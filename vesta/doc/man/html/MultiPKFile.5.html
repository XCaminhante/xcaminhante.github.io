<!-- This file generated automatically by mtex2html(1) -->
<HTML>

<HEAD>
<TITLE>MultiPKFile(5) man page</TITLE>
</HEAD>

<BODY BGCOLOR="#ffffff" VLINK="#006633">
<H2>MultiPKFile(5)</H2>

<H3>Name</H3>
<BLOCKQUOTE>
<P>

<B>MultiPKFile</B> - Vesta-2 cache server stable cache entry file format
</BLOCKQUOTE>

<H3>Contents</H3>
<BLOCKQUOTE>
<P>

<UL>
<LI> <A HREF="MultiPKFile.5.html#IntroSect">Introduction</A>
<LI> <A HREF="MultiPKFile.5.html#GroupingSect">Grouping PKFiles into MultiPKFiles</A>
<LI> <A HREF="MultiPKFile.5.html#GoalsSect">Design Goals and Assumptions</A>
<LI> <A HREF="MultiPKFile.5.html#MPKFileSect">The Structure of MultiPKFiles</A>
<LI> <A HREF="MultiPKFile.5.html#PKFileSect">The Structure of PKFiles</A>
<LI> <A HREF="MultiPKFile.5.html#EntrySect">The Structure of Cache Entries</A>
<LI> <A HREF="MultiPKFile.5.html#SyntaxSect">Syntax Summary</A>
<LI> <A HREF="MultiPKFile.5.html#AnalysisSect">Analysis</A>
<LI> <A HREF="MultiPKFile.5.html#SeeAlsoSect">See Also</A>
<LI> <A HREF="MultiPKFile.5.html#AuthorSect">Author</A>
</UL>
<P>

</BLOCKQUOTE>

<H3><A NAME="IntroSect">Introduction</A></H3>
<BLOCKQUOTE>
<P>

The Vesta-2 cache server's cache entries will be stored persistently
in the repository. To make the lookup process faster, the cache
entries with the same primary key (PK) are grouped together in a
common file. This man page describes the layout of these files
(the so-called "PKFiles") on disk.<P>

Each PK is a 128-bit fingerprint. The Vesta-2 system is being designed
with the goal of servicing up to 60 million cache entries or so.
Hence, the PK space is extremely sparse. Even so, our first
observation was that the number of expected cache entries is too large
for us to store the cache entries for a single PK in a single PKFile.
</BLOCKQUOTE>

<H3><A NAME="GroupingSect">Grouping PKFiles into MultiPKFiles</A></H3>
<BLOCKQUOTE>
<P>

We plan to group the entries for several different PK's together in
repository files (the so-called "MultiPKFiles"). Since the PK's are
fingerprints, and hence (hopefully) random, there is no structure to
the PK's. Therefore, it doesn't matter how we group them for purposes
of achieving better locality. Our plan is to pick three (integer)
values k, k', and m such that (m * k') + k = 128, and divide the bits
of the PK like this:
<PRE>
    k'  k'          k'                 k
  +---+---+- ... -+---+---------------------------------+
  |   |   |       |   |                                 |
  +---+---+- ... -+---+---------------------------------+

  |&lt;---------------------- 128 bits -------------------&gt;|
</PRE>

The m groups of k' bits (the prefix bits) will be used to form the
pathname of a file in the repository, with each group of k' bits
corresponding to an arc of the pathname. Hence, all PK's whose prefix
bits agree will be stored in the same MultiPKFile.<P>

Thus, the important choice here is the value for k. How to divide up
the remaining 128 - k bits (the prefix) into pathname arcs depends on
how the repository implements its directory structures.<P>

We don't have a good analytic way to choose a value for k, since we
don't have enough (any?) data to indicate how PK's and cache entries
will be distributed. Most PK's will have only a single cache entry,
while some will have many. Performance will be best if the
MultiPKFiles are all roughly the same size. Given our design goal of
60M entries and a worst-case estimate of only one entry per PK, using
the values k = 104, k' = 8, and m = 3, we'd expect approximately 4
PK's per MultiPKFile. A larger value for k might be more appropriate.<P>

Since our first choice for k may be sub-optimal, we need to think some
more about how to make the design flexible enough that we can change
the value for k in the future. One possibility would be to use part of
the MultiPKFile namespace to encode the <I>version</I> of the k value.
</BLOCKQUOTE>

<H3><A NAME="GoalsSect">Design Goals and Assumptions</A></H3>
<BLOCKQUOTE>
<P>

The rest of this note sketches the layout of MultiPKFiles on disk. The
goal of the design is to minimize the number of different disk read
operations on a cache Lookup operation, since the disk operations are
the largest source of latency.<P>

We've made several assumptions in formulating our design. First,
MultiPKFiles will be rewritten in their entirety by background cache
server threads. Hence, we haven't focused on designing incremental
structures. Second, since no time-critical path of the cache server is
dependent on the writing of MultiPKFiles, we haven't ruled out designs
that might require fairly large amounts of computation at
MultiPKFile-write time.
</BLOCKQUOTE>

<H3><A NAME="MPKFileSect">The Structure of MultiPKFiles</A></H3>
<BLOCKQUOTE>
<P>

The overall structure of a MultiPKFile is:
<PRE>
   &lt;MultiPKFile&gt;    ::= &lt;Header&gt; &lt;PKFile&gt;*
</PRE>

(We describe the structure of a MultiPKFile by a BNF grammar. In the
grammar that follows, elements in angle brackets denote non-terminals,
and <TT>*</TT> denotes Kleene closure.)<P>

Abstractly, the MultiPKFile <TT>&lt;Header&gt;</TT> provides a map from PK's to the
offsets of the subsequent <TT>&lt;PKFile&gt;</TT>'s in the file.
<PRE>
   &lt;Header&gt;         ::= version num totalLen type-code &lt;TypedHeader&gt;
   &lt;TypedHeader&gt;    ::= &lt;SeqHeader&gt; | &lt;HashHeader&gt;
</PRE>

The <I>version</I> is stored in the <TT>&lt;MultiPKFile&gt;</TT> for backward
compatibility, in case we decide to change some aspect of the
representation in the future. <I>num</I> is the number of entries in the
<TT>&lt;Header&gt;</TT>, as well as the number of <TT>&lt;PKFile&gt;</TT> records
following the <TT>&lt;Header&gt;</TT> in the overall <TT>&lt;MultiPKFile&gt;</TT>.
<I>totalLen</I> is the total length of the <TT>&lt;MultiPKFile&gt;</TT>; it can be
used to compute the length of the last <TT>&lt;PKFile&gt;</TT> in the
<TT>&lt;MultiPKFile&gt;</TT>.<P>

We discussed three ways to represent the <TT>&lt;Header&gt;</TT>. Since we may
want to mix these three schemes, the <TT>&lt;Header&gt;</TT> has an integer
<I>type-code</I> to indicate the format of each header. Here are the three
schemes for representing the <TT>&lt;TypedHeader&gt;</TT>:<P>

<OL>
<LI>
If the number of <TT>&lt;PKFile&gt;</TT>'s in a <TT>&lt;MultiPKFile&gt;</TT> is small, we can
simply list the entries and search them sequentially:
<PRE>
   &lt;SeqHeader&gt;      ::= &lt;HeaderEntry&gt;*
   &lt;HeaderEntry&gt;    ::= &lt;PK&gt; offset
   &lt;PK&gt;             ::= fingerprint
</PRE>

For the <TT>&lt;PK&gt;</TT>, we only have to store the last k bits, since all PK's
in the MultiPKFile will have their first 128 - k bits in common.
The <I>offset</I> value is an integer representing the location of the
corresponding <TT>&lt;PKFile&gt;</TT> relative to the start of the <TT>&lt;MultiPKFile&gt;</TT>.<P>

The PKFile's corresponding to the <TT>&lt;HeaderEntry&gt;</TT> records in a
<TT>&lt;SeqHeader&gt;</TT> are written out in the order in which they appear in
the <TT>&lt;SeqHeader&gt;</TT>. As a result, the <I>offset</I> values are monotonically
increasing, and the length of a PKFile can be computed as the
difference of consecutive <I>offset</I> values. The length of the last
PKFile can be computed using the <I>totalLen</I> value of the <TT>&lt;Header&gt;</TT>.<P>

<LI>
If there are a large number of <TT>&lt;PKFile&gt;</TT>'s within a <TT>&lt;MultiPKFile&gt;</TT>,
we could use the same <TT>&lt;SeqHeader&gt;</TT> representation as in scheme 1,
but store the <TT>&lt;HeaderEntry&gt;</TT>'s in sorted order by <I>pk</I> value. Then
we could do binary search on the <I>pk</I> value to find the offset for
its corresponding <TT>&lt;PKFile&gt;</TT> more quickly.<P>

<LI>
We could use a simple hashing scheme. The hashing scheme could be
parameterized by the table size. If we wanted to get really fancy,
the hash function itself could be parameterized relative to some
family of hash functions and chosen so that there were no
collisions for that table. By our second assumption above, it would
be okay to spend a fair amount of time searching for a perfect hash
function when the MultiPKFile has to be rewritten. Here is one way
to represent the hash table:
<PRE>
   &lt;HashHeader&gt;     ::= tblSize hashKey &lt;Bucket&gt;* &lt;BucketEntry&gt;*
   &lt;Bucket&gt;         ::= 0 | offset
   &lt;BucketEntry&gt;    ::= bucketSize &lt;HeaderEntry&gt;*
</PRE>

The <I>tblSize</I> is the size of the hash table (i.e., the number of
following <TT>&lt;Bucket&gt;</TT> records), and <I>hashKey</I> is a value that
parameterizes the hash function used for this table. The
<TT>&lt;Bucket&gt;</TT>'s are offsets (relative to the start of the
<TT>&lt;HashHeader&gt;</TT>) where the <TT>&lt;BucketEntry&gt;</TT>'s for that bucket
begin, or 0 if the bucket is empty. Each <TT>&lt;BucketEntry&gt;</TT> is some
number of <TT>&lt;HeaderEntry&gt;</TT>'s, prefixed by the number of such entries
(if we use perfect hash functions, the the <I>bucketSize</I> would always
be 1, and could be omitted).
</OL>
<P>

We expect that we will probably use scheme 1 or 2 for the MultiPKFile
<TT>&lt;Header&gt;</TT>.
</BLOCKQUOTE>

<H3><A NAME="PKFileSect">The Structure of PKFiles</A></H3>
<BLOCKQUOTE>
<P>

The overall structure of a <TT>&lt;PKFile&gt;</TT> is:
<PRE>
   &lt;PKFile&gt;         ::= &lt;PKFHeader&gt; &lt;PKEntries&gt;*
   &lt;PKEntries&gt;      ::= numEntries &lt;PKEntry&gt;* &lt;PKEntryExtra&gt;*
</PRE>

The entries in a <TT>&lt;PKFile&gt;</TT> all have the same primary key. They are
also grouped on the disk by a secondary key. There is one
<TT>&lt;PKEntries&gt;</TT> record for each secondary key, and the
<TT>&lt;PKEntry&gt;</TT>'s listed together under a single <TT>&lt;PKEntries&gt;</TT>
record all have the same secondary key. There is one
<TT>&lt;PKEntryExtra&gt;</TT> record for each <TT>&lt;PKEntry&gt;</TT>; they contain extra
values that are not required for the <I>Lookup</I> operation.<P>

What goes into a <TT>&lt;PKEntries&gt;</TT> record? An (abstract) cache entry
has the following fields:<P>

<UL>
<LI> <I>ci</I> - the unique cache index for the entry
<LI> <I>pk</I> - the fingerprint of the cache entry's primary key
<LI> <I>freeVars</I> - the names of the free variables (FV's) for the entry
<LI> <I>fps</I> - the fingerprints of the values of those free variables
<LI> <I>cfp</I> - the combined fingerprint of the <I>common</I> FV's
<LI> <I>ucfp</I> - the combined fingerprint of the <I>uncommon</I> FV's
<LI> <I>value</I> - the output value for the cache entry
</UL>
<P>

The <I>fps</I> values are not needed to perform the <I>Lookup</I>
cache operation, but they are required to recompute the <I>cfp</I> and
<I>ucfp</I> values when the set of <I>common</I> FV's for the <TT>&lt;PKFile&gt;</TT>
changes (see below).<P>

According to the cache server specification, the cache also stores
certain information in the <TT>&lt;PKFile&gt;</TT> that applies to all its
entries. This information is stored in the <TT>&lt;PKFHeader&gt;</TT>:<P>

<UL>
<LI>
<I>allNames</I> -
the union of the FV's for the <TT>&lt;PKFile&gt;</TT>'s entries
<LI>
<I>commonNames</I> -
a bit vector (relative to <I>allNames</I>) representing the
set of FV's common to all the <TT>&lt;PKFile&gt;</TT>'s entries
<LI>
<I>epoch</I> -
an integer representing the "version" of the <I>allNames</I> list
</UL>
<P>

The <I>freeVars</I> field within an entry is actually not stored
explicitly. Instead, the names of each entry's free variables are
stored implicitly by referencing the <I>allNames</I> field of the
<TT>&lt;PKFile&gt;</TT>. The common names need not be stored, since they are
represented by the <I>commonNames</I> bit vector in the <TT>&lt;PKFHeader&gt;</TT>.
The remaining <I>uncommon</I> names are stored as a bit vector in the
<TT>&lt;PKEntry&gt;</TT>; this bit vector is also interpreted relative to the
<I>allNames</I> field, and differs from one entry to the next.<P>

To check for a cache hit, the cache server computes the combined
fingerprint of the fingerprints of the <I>common</I> FV's in the current
evaluation environment. It compares this computed <I>common</I>
fingerprint to the <I>cfp</I> fields of the entries. For each entry that
matches, it computes a combined <I>uncommon</I> fingerprint for each of
the free variables in the current evaluation environment matching the
entry's <I>uncommon</I> names. If both the <I>cfp</I> and <I>ucfp</I> tests
succeed, the corresponding <I>value</I> is returned.<P>

What's important to notice is that the <I>cfp</I> test can be made
without any per-entry computation. Also, the only per-entry data
required for this test is the value of the entry's <I>cfp</I> field. By
contrast, the <I>ucfp</I> test requires the server to read both the
<I>uncommonNames</I> bit vector and the <I>ucfp</I> values from the entry,
and to compute a combined <I>uncommon</I> fingerprint. So the <I>ucfp</I>
test is more expensive.<P>

We decided the the entry's <I>cfp</I> fields should also be stored in the
<TT>&lt;PKFHeader&gt;</TT>. The <I>cfp</I> computed for the <I>common</I> free variables
in the current evaluation environment thus serves as the secondary
search key. Each of these fingerprints is 16 bytes, so the hope is
that the <TT>&lt;PKFHeader&gt;</TT> will still be small enough that it can be
read by a single disk operation.<P>

Here is the layout of the <TT>&lt;PKFHeader&gt;</TT>:
<PRE>
   &lt;PKFHeader&gt;      ::= &lt;CFPHeader&gt; &lt;PKFHeaderTail&gt;
   &lt;CFPHeader&gt;      ::= num type-code &lt;CFPTypedHeader&gt;
   &lt;PKFHeaderTail&gt;  ::= pkEpoch namesEpoch &lt;AllNames&gt; &lt;CommmonNames&gt;
   &lt;AllNames&gt;       ::= numNames &lt;Name&gt;*
   &lt;Name&gt;           ::= nameLen byte*
   &lt;CommonNames&gt;    ::= &lt;BitVector&gt;
   &lt;BitVector&gt;      ::= numWords word*
</PRE>

As before, there are several possibilities for the design of the
<TT>&lt;TypedCFPHeader&gt;</TT>. Abstractly, the <TT>&lt;CFPHeader&gt;</TT> is a table
that maps the common fingerprint value (16 bytes) to a <I>list</I> of cache
entries with the same PK and <I>cfp</I> values. These designs are similar
to the 3 <TT>&lt;TypedHeader&gt;</TT> designs above, except that the schemes are
generalized to accomodate lists of entries.
<PRE>
   &lt;CFPTypedHeader&gt; ::= &lt;CFPSeqHeader&gt; | &lt;CFPHashHeader&gt;
</PRE>

<OL>
<LI>
If the number of entries in a PKFile is small, we can simply search
them sequentially, using the <TT>&lt;SeqHeader&gt;</TT> structure described above.
<PRE>
   &lt;CFPSeqHeader&gt;   ::= &lt;CFPHeaderEntry&gt;*
   &lt;CFPHeaderEntry&gt; ::= &lt;CFP&gt; offset
   &lt;CFP&gt;            ::= fingerprint
</PRE>

A <TT>&lt;CFPSeqHeader&gt;</TT> consists of <I>num</I> <TT>&lt;CFPHeaderEntry&gt;</TT>
records. The <I>offset</I> of a <TT>&lt;CFPHeaderEntry&gt;</TT> is the offset
where its corresponding <TT>&lt;PKEntries&gt;</TT> record begins. The
<I>offset</I> is taken relative to the start of either the
<TT>&lt;PKFHeader&gt;</TT> or the <TT>&lt;CFPHeader&gt;</TT> (whichever we think is easier
to compute against).<P>

<LI>
If a PKFile has several entries, we can use the <TT>&lt;CFPSeqHeader&gt;</TT>
representation, but store the <TT>&lt;CFPEntry&gt;</TT>'s in sorted order by
<I>cfp</I>. This would allow us to use binary search on the table.<P>

<LI>
Another alternative when there are lots of entries with the same PK
is to use a hash table, using the <TT>&lt;HashHeader&gt;</TT> structure described
above:
<PRE>
   &lt;CFPHashHeader&gt;  ::= len tblSize hashKey &lt;Bucket&gt;* &lt;CFPBucketEntry&gt;*
   &lt;Bucket&gt;         ::= 0 | offset
   &lt;CFPBucketEntry&gt; ::= bucketSize &lt;CFPHeaderEntry&gt;*
</PRE>

The <I>len</I> field in the <TT>&lt;CFPHashHeader&gt;</TT> is the total size of
the <TT>&lt;CFPHashHeader&gt;</TT>, so we can seek past the <TT>&lt;CFPHeader&gt;</TT> to
the <TT>&lt;AllNames&gt;</TT> record without having to inspect the
<I>bucketSize</I> values of the individual <TT>&lt;CFPBucketEntry&gt;</TT>'s.
</OL>
<P>

We expect that we will use scheme 2 or 3 for the <TT>&lt;CFPHeader&gt;</TT>.
</BLOCKQUOTE>

<H3><A NAME="EntrySect">The Structure of Cache Entries</A></H3>
<BLOCKQUOTE>
<P>

All that's left is to describe the format of the cache entries
themselves. The data in a cache entry is split between the
<TT>&lt;PKEntry&gt;</TT> data required to perform the <I>Lookup</I> operation, and
the <TT>&lt;PKEntryExtra&gt;</TT> information required when MultiPKFiles are
rewritten.<P>

Here is the structure of a <TT>&lt;PKEntry&gt;</TT> record:
<PRE>
   &lt;PKEntry&gt;        ::= ci &lt;UncommonNames&gt; &lt;UCFP&gt; &lt;Value&gt;
   &lt;UncommonNames&gt;  ::= &lt;BitVector&gt;
   &lt;UCFP&gt;           ::= xortag fingerprint
   &lt;Value&gt;          ::= numBytes byte*
</PRE>

The <I>ci</I> and <I>numBytes</I> fields are integers, and the <I>xortag</I> is a
one-word tag formed by XOR'ing together the uncommon fingerprints. The
<TT>&lt;UncommonNames&gt;</TT> bit vector is interpreted relative to the
<TT>&lt;AllNames&gt;</TT> record of the corresponding <TT>&lt;PKFHeader&gt;</TT>. The
<TT>&lt;Value&gt;</TT> is simply a sequence of <I>numBytes</I> bytes.<P>

Here is the structure of a <TT>&lt;PKEntryExtra&gt;</TT> record:
<PRE>
   &lt;PKEntryExtra&gt;   ::= numNames index* fingerprint*
</PRE>

The <TT>&lt;PKEntryExtra&gt;</TT> encodes the <I>fps</I> field of an entry
(the fingerprints of the entry's free variables). The
<I>numNames</I> value is an integer, followed by three lists each of
<I>numNames</I> elements. The <I>index</I> values are integers representing the
inverse of the mapping from the entry's PKFile header <TT>&lt;AllNames&gt;</TT>
indices to indices in the <I>fingerprint</I> list. If we denote
the <I>j</I>th element of a list <I>list</I> by <I>list[j]</I>, then for all values
<I>i</I> in the half-open interval [0, <I>numNames</I>), the fingerprint
of the name <I>AllNames[index[i]]</I> is <I>fingerprint[i]</I>
</BLOCKQUOTE>

<H3><A NAME="SyntaxSect">Syntax Summary</A></H3>
<BLOCKQUOTE>
<P>

For reference, here is the complete MultiPKFile grammar:
<PRE>
   &lt;MultiPKFile&gt;    ::= &lt;Header&gt; &lt;PKFile&gt;*

   &lt;Header&gt;         ::= version num totalLen type-code &lt;TypedHeader&gt;
   &lt;TypedHeader&gt;    ::= &lt;SeqHeader&gt; | &lt;HashHeader&gt;
   &lt;SeqHeader&gt;      ::= &lt;HeaderEntry&gt;*
   &lt;HashHeader&gt;     ::= tblSize hashKey &lt;Bucket&gt;* &lt;BucketEntry&gt;*
   &lt;Bucket&gt;         ::= 0 | offset
   &lt;BucketEntry&gt;    ::= bucketSize &lt;HeaderEntry&gt;*
   &lt;HeaderEntry&gt;    ::= &lt;PK&gt; offset
   &lt;PK&gt;             ::= fingerprint

   &lt;PKFile&gt;         ::= &lt;PKFHeader&gt; &lt;PKEntries&gt;*

   &lt;PKFHeader&gt;      ::= &lt;CFPHeader&gt; &lt;PKFHeaderTail&gt;
   &lt;CFPHeader&gt;      ::= num type-code &lt;CFPTypedHeader&gt;
   &lt;PKFHeaderTail&gt;  ::= pkEpoch namesEpoch &lt;AllNames&gt; &lt;CommmonNames&gt;
   &lt;CFPTypedHeader&gt; ::= &lt;CFPSeqHeader&gt; | &lt;CFPHashHeader&gt;
   &lt;CFPSeqHeader&gt;   ::= &lt;CFPHeaderEntry&gt;*
   &lt;CFPHashHeader&gt;  ::= len tblSize hashKey &lt;Bucket&gt;* &lt;CFPBucketEntry&gt;*
   &lt;Bucket&gt;         ::= 0 | offset
   &lt;CFPBucketEntry&gt; ::= bucketSize &lt;CFPHeaderEntry&gt;*
   &lt;CFPHeaderEntry&gt; ::= &lt;CFP&gt; offset
   &lt;CFP&gt;            ::= fingerprint
   &lt;AllNames&gt;       ::= numNames &lt;Name&gt;*
   &lt;Name&gt;           ::= nameLen byte*
   &lt;CommonNames&gt;    ::= &lt;BitVector&gt;
   &lt;BitVector&gt;      ::= numWords word*

   &lt;PKEntries&gt;      ::= numEntries &lt;PKEntry&gt;* &lt;PKEntryExtra&gt;*
   &lt;PKEntry&gt;        ::= ci &lt;UncommonNames&gt; &lt;UCFP&gt; &lt;Value&gt;
   &lt;UncommonNames&gt;  ::= &lt;BitVector&gt;
   &lt;UCFP&gt;           ::= xortag fingerprint
   &lt;Value&gt;          ::= numBytes byte*
   &lt;PKEntryExtra&gt;   ::= numNames index* fingerprint*
</PRE>

</BLOCKQUOTE>

<H3><A NAME="AnalysisSect">Analysis</A></H3>
<BLOCKQUOTE>
<P>

To determine how well this design meets our goal of minimizing the
number of different disk reads per <I>Lookup</I>, we must first estimate
the sizes of the various data records.<P>

First, consider the <TT>&lt;Header&gt;</TT> record. Most of its space is devoted
to the <TT>&lt;HeaderEntry&gt;</TT>'s, which are at most 20 bytes (16 bytes for
the <I>pk</I> and 4 bytes for the <I>offset</I>). So, the size of the
<TT>&lt;Header&gt;</TT> is roughly "20 * n", where <I>n</I> is the number of
<TT>&lt;PKFile&gt;</TT>'s in the <TT>&lt;MultiPKFile&gt;</TT>. If the <TT>&lt;HashHeader&gt;</TT>
representation is used, then some more bytes are required by the
<TT>&lt;Bucket&gt;</TT>'s. If each <I>offset</I> is 2 bytes and the table is only
half loaded, then "4 * n" bytes are required for the <TT>&lt;Bucket&gt;</TT>'s.<P>

The operating system buffers disk operations, transferring data in
blocks on the order of 8K bytes in size. So, with a single disk
operation, we can do a search on the <TT>&lt;Header&gt;</TT> using one disk read
if the MultiPKFile contains at most on the order of 340 PKFiles. We
don't expect to have nearly that many PKFile's per MultiPKFile.<P>

On to the <TT>&lt;PKFHeader&gt;</TT>. As with the <TT>&lt;Header&gt;</TT>, most of the
space is devoted to the <TT>&lt;CFPHeader&gt;</TT> table mapping common
fingerprints to the offsets of the corresponding <TT>&lt;PKEntries&gt;</TT>
records. These tables have the same size as the <TT>&lt;Header&gt;</TT> records,
about "24 * n", where <I>n</I> is the number of entries in the table. In
this case, however, we expect <I>n</I> to be quite a bit larger, so
multiple disk reads may be required to complete this part of the
<I>Lookup</I> operation. By storing the <I>epoch</I> value first, the
cache can report an <I>FVMismatch</I> result without doing further disk
reads.<P>

The <TT>&lt;AllNames&gt;</TT> record may be quite long -- perhaps on the order
of 600 bytes or more, assuming 30 20-character names. We might be able
to do some simple compression on the names in an <TT>&lt;AllNames&gt;</TT>
record, since we expect that many of them will share common prefixes.<P>

Since the <TT>&lt;AllNames&gt;</TT> and <TT>&lt;CommonNames&gt;</TT> records only have to
be read when those values have not been stored in the cache's
in-memory data structures, it seems a shame to interpose them between
the <TT>&lt;CFPHeader&gt;</TT> and the <TT>&lt;PKEntries&gt;</TT>. We might be able to
reduce the number of disk reads in some cases by storing the
<TT>&lt;AllNames&gt;</TT> and <TT>&lt;CommonNames&gt;</TT> records after the
<TT>&lt;PKEntries&gt;</TT>, at the expense of an extra field in the
<TT>&lt;PKFHeader&gt;</TT> indicating where the <TT>&lt;AllNames&gt;</TT> record begins.<P>

The <TT>&lt;PKEntry&gt;</TT>'s themselves are relatively small. The <I>ci</I>,
<TT>&lt;UncommonNames&gt;</TT>, and <TT>&lt;UCFP&gt;</TT> records require about 28 bytes.
The <TT>&lt;Value&gt;</TT> record varies from entry to entry. Most <TT>&lt;Value&gt;</TT>
records will be pointers to derived files in the repository, requiring
only 20 bytes. But a small fraction of them will be pickled Vesta
values, which can be quite large. Since the <TT>&lt;Value&gt;</TT> records are
only required in the event of a cache hit, it might be better to store
all of the <TT>&lt;Value&gt;</TT> records together after the <TT>&lt;PKEntry&gt;</TT>'s at
the cost of an extra integer offset in the <TT>&lt;PKEntries&gt;</TT> record
indicating where the <TT>&lt;Value&gt;</TT> records begin.<P>

In a similar vein, notice how the <TT>&lt;PKEntryExtra&gt;</TT>'s have been
stored after the <TT>&lt;PKEntry&gt;</TT>'s. This is because the
<TT>&lt;PKEntryExtra&gt;</TT>'s are not required on the <I>Lookup</I> path; storing
their values within the <TT>&lt;PKEntry&gt;</TT>'s would hurt data locality and
increase the number of disk reads required per <I>Lookup</I>.<P>

The net result of this design is that we expect that misses in the
cache can be determined in as few as 2 disk reads, but usually 3. We
expect hits in the cache to require 4 or more disk reads.
</BLOCKQUOTE>

<H3><A NAME="SeeAlsoSect">See Also</A></H3>
<BLOCKQUOTE>
<P>

<A HREF="PrintMPKFile.1.html">PrintMPKFile(1)</A>,
<A HREF="VCache.1.html">VCache(1)</A>
</BLOCKQUOTE>

<H3><A NAME="AuthorSect">Author</A></H3>
<BLOCKQUOTE>
<P>

Allan Heydon
(<TT>caheydon@yahoo.com</TT>)
<PRE>
Last modified on Thu Nov  8 12:48:54 EST 2001 by ken@xorian.net
     modified on Sun Apr  6 18:10:03 PDT 1997 by heydon
</PRE>

</BLOCKQUOTE>

This page was generated automatically by
<A HREF="../../../mtex/index.html">mtex software</A>.
</BODY>
</HTML>
